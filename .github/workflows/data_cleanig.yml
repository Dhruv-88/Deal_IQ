name: Deal Prediction Pipeline

# Trigger the workflow
on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual trigger

# Set global environment variables
env:
  PYTHON_VERSION: '3.12'

jobs:
  # ==================== STAGE 1: SETUP ====================
  setup:
    name: "Stage 1: Environment Setup"
    runs-on: ubuntu-latest
    
    outputs:
      python-version: ${{ env.PYTHON_VERSION }}
      cache-hit: ${{ steps.cached-poetry-dependencies.outputs.cache-hit }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
      id: setup-python
    
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true
        installer-parallel: true
    
    - name: Load cached dependencies
      id: cached-poetry-dependencies
      uses: actions/cache@v3
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}
    
    - name: Install dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: |
        echo "Installing Python dependencies..."
        poetry install --no-interaction --no-root
        echo "✓ Dependencies installed successfully"
    
    - name: Verify setup
      run: |
        echo "Verifying Python environment..."
        poetry run python --version
        poetry run python -c "import pandas, pyarrow; print('Core packages available')"
        echo "✓ Stage 1 (Setup) completed successfully"

  # ==================== STAGE 2: GOOGLE CLOUD AUTH ====================
  google-auth:
    name: "Stage 2: Google Cloud Authentication"
    runs-on: ubuntu-latest
    needs: setup
    
    outputs:
      auth-status: ${{ steps.auth-test.outputs.status }}
      project-id: ${{ steps.set-vars.outputs.project-id }}
      bucket-name: ${{ steps.set-vars.outputs.bucket-name }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python (from cache)
      uses: actions/setup-python@v4
      with:
        python-version: ${{ needs.setup.outputs.python-version }}
    
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true
    
    - name: Restore cached dependencies
      uses: actions/cache@v3
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ needs.setup.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}
    
    - name: Install dependencies (ensure they exist)
      run: |
        echo "Ensuring dependencies are installed..."
        poetry install --no-interaction --no-root
        echo "Verifying core packages..."
        poetry run python -c "import pandas, pyarrow; print('Dependencies verified successfully')"
    
    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}
        project_id: ${{ secrets.GCP_PROJECT_ID }}
    
    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v2
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
    
    - name: Set environment variables
      id: set-vars
      run: |
        echo "GCP_PROJECT_ID=${{ secrets.GCP_PROJECT_ID }}" >> $GITHUB_ENV
        echo "GCS_BUCKET_NAME=${{ secrets.GCS_BUCKET_NAME }}" >> $GITHUB_ENV
        echo "project-id=${{ secrets.GCP_PROJECT_ID }}" >> $GITHUB_OUTPUT
        echo "bucket-name=${{ secrets.GCS_BUCKET_NAME }}" >> $GITHUB_OUTPUT
    
    - name: Test Google Cloud authentication
      id: auth-test
      run: |
        echo "Testing Google Cloud authentication..."
        
        # Test gcloud auth
        echo "Testing gcloud authentication:"
        gcloud auth list
        gcloud config list project
        
        # Test GCS access
        echo "Testing GCS bucket access:"
        if gsutil ls gs://${{ secrets.GCS_BUCKET_NAME }}; then
          echo "✓ Bucket access successful"
        else
          echo "! Bucket might be empty or doesn't exist yet"
        fi
        
        echo "status=success" >> $GITHUB_OUTPUT
      env:
        GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }}
    
    - name: Test Python GCS operations
      run: |
        echo "Testing Python GCS operations..."
        poetry run python scripts/test_auth.py
      env:
        GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }}
    
    - name: Authentication stage complete
      run: |
        echo "✓ Stage 2 (Google Cloud Authentication) completed successfully"
        echo "Project ID: ${{ secrets.GCP_PROJECT_ID }}"
        echo "Bucket: ${{ secrets.GCS_BUCKET_NAME }}"

  # ==================== STAGE 3: DATA CLEANING ====================
  data-cleaning:
    name: "Stage 3: Data Cleaning"
    runs-on: ubuntu-latest
    needs: [setup, google-auth]
    
    outputs:
      cleaning-status: ${{ steps.run-cleaning.outputs.status }}
      dataset-shape: ${{ steps.run-cleaning.outputs.dataset-shape }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python (from cache)
      uses: actions/setup-python@v4
      with:
        python-version: ${{ needs.setup.outputs.python-version }}
    
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true
    
    - name: Restore cached dependencies
      uses: actions/cache@v3
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ needs.setup.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}
    
    - name: Install dependencies (ensure they exist)
      run: |
        echo "Ensuring dependencies are installed..."
        poetry install --no-interaction --no-root
        echo "Verifying core packages..."
        poetry run python -c "import pandas, pyarrow; print('Dependencies verified successfully')"
    
    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}
        project_id: ${{ secrets.GCP_PROJECT_ID }}
    
    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v2
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
    
    - name: Set environment variables for data cleaning
      run: |
        echo "Setting up environment variables for data cleaning..."
        echo "PROJECT_ID=${{ secrets.GCP_PROJECT_ID }}" >> $GITHUB_ENV
        echo "BUCKET_NAME=${{ secrets.GCS_BUCKET_NAME }}" >> $GITHUB_ENV
        echo "GCP_PROJECT_ID=${{ secrets.GCP_PROJECT_ID }}" >> $GITHUB_ENV
        echo "GCS_BUCKET_NAME=${{ secrets.GCS_BUCKET_NAME }}" >> $GITHUB_ENV
        echo "✓ Environment variables configured"
    
    - name: Verify data cleaning dependencies
      run: |
        echo "Verifying data cleaning module dependencies..."
        poetry run python -c "
        try:
            import sys
            import os
            sys.path.append(os.getcwd())
            from DataCleaning.data_cleaning_main import main_data_cleaning_pipeline
            print('✓ Data cleaning module imported successfully')
        except ImportError as e:
            print(f'❌ Import error: {e}')
            sys.exit(1)
        except Exception as e:
            print(f'❌ Error: {e}')
            sys.exit(1)
        "
    
    - name: Run data cleaning pipeline
      id: run-cleaning
      run: |
        echo "==================== STARTING DATA CLEANING PIPELINE ===================="
        echo "Project ID: $PROJECT_ID"
        echo "Bucket Name: $BUCKET_NAME"
        echo ""
        
        # Create a wrapper script to handle the data cleaning and capture outputs
        cat > run_cleaning.py << 'EOF'
        import sys
        import os
        import traceback
        
        try:
            # Add current directory to Python path
            sys.path.append(os.getcwd())
            
            # Import and run the data cleaning pipeline
            from DataCleaning.data_cleaning_main import main_data_cleaning_pipeline
            
            print("🚀 Starting data cleaning pipeline...")
            print("-" * 60)
            
            # Run the main data cleaning pipeline
            cleaned_df = main_data_cleaning_pipeline()
            
            print("-" * 60)
            print(f"✅ Data cleaning completed successfully!")
            print(f"📊 Final dataset shape: {cleaned_df.shape}")
            print(f"📈 Rows: {cleaned_df.shape[0]:,}")
            print(f"📋 Columns: {cleaned_df.shape[1]}")
            
            # Print final column info
            print("\n📋 Final columns:")
            for i, col in enumerate(cleaned_df.columns, 1):
                print(f"  {i:2d}. {col}")
            
            # Print data quality summary
            print(f"\n📊 Data Quality Summary:")
            total_cells = cleaned_df.shape[0] * cleaned_df.shape[1]
            null_cells = cleaned_df.isnull().sum().sum()
            completeness = ((total_cells - null_cells) / total_cells) * 100
            print(f"  • Total cells: {total_cells:,}")
            print(f"  • Null cells: {null_cells:,}")
            print(f"  • Data completeness: {completeness:.2f}%")
            
            # Save outputs for GitHub Actions
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write(f"status=success\n")
                f.write(f"dataset-shape={cleaned_df.shape[0]}x{cleaned_df.shape[1]}\n")
            
            print("\n🎉 DATA CLEANING PIPELINE COMPLETED SUCCESSFULLY! 🎉")
            
        except Exception as e:
            print(f"\n❌ ERROR in data cleaning pipeline:")
            print(f"Error type: {type(e).__name__}")
            print(f"Error message: {str(e)}")
            print(f"\nFull traceback:")
            traceback.print_exc()
            
            # Save error status for GitHub Actions
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write(f"status=failed\n")
                f.write(f"dataset-shape=error\n")
            
            sys.exit(1)
        EOF
        
        # Run the data cleaning pipeline
        poetry run python run_cleaning.py
      env:
        PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }}
        GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }}
    
    - name: Data cleaning stage summary
      run: |
        echo "==================== DATA CLEANING STAGE SUMMARY ===================="
        echo "Status: ${{ steps.run-cleaning.outputs.status }}"
        echo "Dataset Shape: ${{ steps.run-cleaning.outputs.dataset-shape }}"
        echo ""
        if [ "${{ steps.run-cleaning.outputs.status }}" = "success" ]; then
          echo "✅ Stage 3 (Data Cleaning) completed successfully!"
          echo "🎯 Ready for next pipeline stage"
        else
          echo "❌ Stage 3 (Data Cleaning) failed!"
          echo "🔍 Check the logs above for error details"
          exit 1
        fi
        echo "====================================================================="

  # ==================== STAGE 4: MODEL TRAINING (PLACEHOLDER) ====================
  model-training:
    name: "Stage 4: Model Training"
    runs-on: ubuntu-latest
    needs: [setup, google-auth, data-cleaning]
    if: needs.data-cleaning.outputs.cleaning-status == 'success'
    
    steps:
    - name: Model training placeholder
      run: |
        echo "==================== MODEL TRAINING STAGE ===================="
        echo "✅ Data cleaning completed successfully"
        echo "📊 Dataset shape: ${{ needs.data-cleaning.outputs.dataset-shape }}"
        echo ""
        echo "🚧 Model training stage - Coming next..."
        echo "This stage will include:"
        echo "  • Feature engineering"
        echo "  • Model training"
        echo "  • Model validation"
        echo "  • Model deployment"
        echo "=============================================================="